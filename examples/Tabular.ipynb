{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `barusini.tabular`\n",
    "This notebook provides a walk-through of the `barusini.tabular` python package, which is meant to help with creating ML Pipelines combining advanced feature engineering and modeling techniques for tabular data. \n",
    "\n",
    "**The following topics are covered:**\n",
    "\n",
    "\n",
    "- 1. [Introduction](#1.-Introduction)\n",
    "\n",
    "- 2. [Custom ML Pipeline](#2.-Custom-ML-Pipeline)\n",
    "\n",
    "- 3. [Feature Engineering Search](#3.-Feature-Engineering-Search)\n",
    "\n",
    "- 4. [Model Hyper-Parameter Tunning](#4.-Tune-Hyper-Parameters-of-Pipeline's-Model)\n",
    "\n",
    "- 5. [Ensembling](#5.-Ensembling-Multiple-Pipelines)\n",
    "\n",
    "- 6. [Model Comparison](#6.-Model-Comparison)\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "Creating ML models or pipelines consists of multiple stages, exploratory data analysis, data preprocessing, feature engineering, model hyperparameter tuning and potentially ensembling. There are many python libraries that help with one or more of these tasks, however, often applying some of the advanced ML techniques requires writing lot of boilerplate code. \n",
    "\n",
    "This package aims to help with data preprocessing, feature engineering, hyperparameter tuning and ensembling, leveraging known python packages that are great at doing what they are supposed to do. It works with `pandas.DataFrame` and `pandas.Series` objects, which are always expected to have proper column names. This makes everything easier to understand and debug. For validation, metric computation and even modeling, `scikit-learn` does most of the heavy lifting. For modeling, any python package that has scikit-learn like API (`fit`/ `predict`/`predict_proba`) can be used, this notebook also uses packages `xgboost` and `lightgbm` to demonstrate the compatibility. For hyper-parameter search, `optuna` is used.\n",
    "\n",
    "For proper implementation and validation of some advanced ML techniques (like mean target encoding) data preprocessing/feature engineering and modeling have to be bundled in a single object, which makes sure the model fitting and validation are both done properly and are not leaking any target information (overfitting). In the context of this package it is called pipeline.\n",
    "## 1.1 Pipeline\n",
    "\n",
    "ML Pipelines are defined by the `Pipeline` object. Pipelines consists of 2 main components:\n",
    "- **transformers** - defining data preprocessing/feature engineering\n",
    "- **model** - defines the ML algorithm with it's hyper-parameters\n",
    "\n",
    "## 1.2 Transformers\n",
    "Transformers define the data transformations that will be applied to the input data. They are applied serially in the order in which they are defined. Transformers are always initialized with keyword `used_cols=cols_to_use` where `cols_to_use` is a list of one or more columns that are either part of the original data, or are created during applying transformers defined previously in a list.\n",
    "\n",
    "\n",
    "In this example we are using following transformers:\n",
    "\n",
    "- `Identity` - take one or more columns from the data and use them as they are (select this variable for a model)\n",
    "- `CustomOneHotEncoder` - one hot encode one or more input columns, the vector size will be the number of all unique combinations of input column values that appear in the data\n",
    "- `CustomLabelEncoder` - label encode one or more input columns, it will have as many different values as there are unique combinations of input column values that appear in the data\n",
    "- `MeanTargetEncoder` - encode one or more input columns by their mean response/target/label value, it is done in CV-fashion to avoid leaking the target\n",
    "\n",
    "## 1.3 Model\n",
    "\n",
    "Model is your standard scikit-learn API like model, such as sklearn Random Forest, xgboost, lightgbm and so on. This library does not implement any models, but should be compatible with any other library with scikit-learn like API where models implement these functions:\n",
    "\n",
    "- `fit` - fitting the model\n",
    "- `predict` - predicts the hard label\n",
    "- `predict_proba` - predicts the soft label (probabilities) for classification models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom ML Pipeline\n",
    "\n",
    "In this chapter, we will see how a custom ML Pipeline can be defined, validated, fitted, evaluated, used to transform datasets and how we can see the most important features of the Pipeline.\n",
    "\n",
    "## 2.1 Imports\n",
    "In addition to `barusini.tabular` we will also use these known packages:\n",
    "\n",
    "- **copy** - used to copy/clone our models and pipelines\n",
    "- **pandas** - pipelines work with pandas DataFrames and Series\n",
    "- **numpy** - used for numeric operations\n",
    "- **sklearn** - used for validation, metric computation and modeling\n",
    "- **xgboost** - used for modeling\n",
    "- **lightgbm** - used for modeling\n",
    "- **optuna** - is not imported, but it used internally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: stdin isn't a terminal\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from barusini.tabular import feature_engineering, model_search, auto_ml\n",
    "from barusini.tabular.stages.hyper_parameter_tuning import LOG, LOGINT, UNIFORM\n",
    "from barusini.tabular.transformers import CustomLabelEncoder, CustomOneHotEncoder, MeanTargetEncoder, Pipeline, Ensemble, Identity, WeightedAverage\n",
    "\n",
    "from barusini.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loading The Data\n",
    "\n",
    "In this notebook we will be using Adult dataset https://archive.ics.uci.edu/ml/datasets/adult where the goal is to predict if people's early income exceeds $50 000. The dataset is already split into training and test. We will create following variables:\n",
    "\n",
    "- `target` - name of the target/label column\n",
    "- `train_X` - pd.DataFrame, training split without the target column\n",
    "- `train_y` - pd.Series, training target\n",
    "- `test_X` - pd.DataFrame, test split without the target column\n",
    "- `test_y` - pd.Series, test target\n",
    "\n",
    "\n",
    "In practice we don't need to drop the target column from training/test data frames when using `barusini`, however, it is a good practice to work with the training data without the label to avoid unintentional target leaks in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  target  \n",
       "0          2174             0              40   United-States       0  \n",
       "1             0             0              13   United-States       0  \n",
       "2             0             0              40   United-States       0  \n",
       "3             0             0              40   United-States       0  \n",
       "4             0             0              40            Cuba       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"target\",\n",
    "]\n",
    "\n",
    "train_data = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "train = pd.read_csv(train_data, header=None, names=columns)\n",
    "\n",
    "test_data = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "test = pd.read_csv(test_data, header=None, names=columns, skiprows=1)\n",
    "\n",
    "target = \"target\"\n",
    "train[target] = train[target].str.contains(\">\").astype(int)\n",
    "test[target] = test[target].str.contains(\">\").astype(int)\n",
    "\n",
    "train_X = train.drop(columns=[target])\n",
    "test_X = test.drop(columns=[target])\n",
    "\n",
    "train_y = train[target]\n",
    "test_y = test[target]\n",
    "\n",
    "display(train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Pipeline Definition\n",
    "\n",
    "Let's define our first Pipeline. We will use numeric columns `age`,` education-num`, `capital-gain`, `capital-loss` and `hours-per-week` as they are. For string column `marital-status` we will use mean target encoding, for `sex` we will use one-hot encoding and for `workclass` we will use label encoding to demonstrate different encoding techniques. For modeling we will use random forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline (8 Transformers):\n",
      "    * Identity: ['age']\n",
      "    * Identity: ['education-num']\n",
      "    * Identity: ['capital-gain']\n",
      "    * Identity: ['capital-loss']\n",
      "    * Identity: ['hours-per-week']\n",
      "    * Target encoder for feature '['marital-status']'\n",
      "          * Unfitted Transformer\n",
      "    * One Hot Encoder for feature '['sex']':\n",
      "      \tCategories: Unfitted Transformer\n",
      "    * Label Encoder for feature '['workclass']':\n",
      "      \tCategories: Unfitted Transformer\n",
      "    * RandomForestClassifier(max_depth=7, random_state=42)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model = Pipeline(\n",
    "    transformers=[\n",
    "        Identity(used_cols=[\"age\"]),\n",
    "        Identity(used_cols=[\"education-num\"]),\n",
    "        Identity(used_cols=[\"capital-gain\"]),\n",
    "        Identity(used_cols=[\"capital-loss\"]),\n",
    "        Identity(used_cols=[\"hours-per-week\"]),\n",
    "        MeanTargetEncoder(used_cols=[\"marital-status\"]),\n",
    "        CustomOneHotEncoder(used_cols=[\"sex\"]),\n",
    "        CustomLabelEncoder(used_cols=[\"workclass\"]),\n",
    "    ],\n",
    "    model = RandomForestClassifier(max_depth=7, random_state=42),\n",
    ")\n",
    "\n",
    "print(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 (Cross) Validation\n",
    "`barusini` implements convenient method `cross_val_score` for computing validation scores, that allows the user to pass in scikit-learn validation objects. This example uses `sklearn.model_selection.StratifiedKFold` validation object.\n",
    "\n",
    "Our method for computing validation scores takes these parameters:\n",
    "- `model` - `Pipeline` object\n",
    "- `X` - pd.DataFrame - input data\n",
    "- `y` - pd.Series - input target\n",
    "- `cv` - scikit-learn like validation object\n",
    "- `scoring` - scikit-learn like metric function\n",
    "- `n_jobs` - int - number of processes to use, we can speed up validation by providing larger number, each validation split can be evaluated in separate process\n",
    "- `proba` - bool - whether we should use `predict_proba` or `predict` function of the `model`\n",
    "\n",
    "In this example, we use Area Under Curve for Receiver Operating Characteristic (ROC AUC) score. This score is useful when we don't need the output probabilities to be calibrated but we want our predictions to be ranked/order properly (higher probability really means an event is more likely, but score 0.9 does not necessarily need mean probability of an event is 90%). Baseline ROC AUC (score of random predictions) is 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC: 0.9115\n"
     ]
    }
   ],
   "source": [
    "rf_cv_auc = cross_val_score(\n",
    "    model=rf_model,\n",
    "    X=train_X,\n",
    "    y=train_y,\n",
    "    cv=StratifiedKFold(),\n",
    "    scoring=roc_auc_score,\n",
    "    n_jobs=1,\n",
    "    proba=True,\n",
    ")\n",
    "print(\"CV AUC:\", round(rf_cv_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Pipeline evaluation\n",
    "\n",
    "After we estimated what our pipeline performance should be, we can evaluate it on actual test data. We will firstly use pipeline's `fit` method to fit our model on full training data, then `predict_proba` method to predict the probabilities. Since this is a binary classification problem, our model will output vector of size 2 when making predictions, these will be the probabilities of the two classes. For computing ROC AUC, we will only use the probability of event happening (person earning more than 50K). Afterwards, we can print the fitted pipeline and the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline (8 Transformers):\n",
      "    * Identity: ['age']\n",
      "    * Identity: ['education-num']\n",
      "    * Identity: ['capital-gain']\n",
      "    * Identity: ['capital-loss']\n",
      "    * Identity: ['hours-per-week']\n",
      "    * Target encoder for feature '['marital-status']'\n",
      "          * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                            ('mean', LinearRegression())])\n",
      "    * One Hot Encoder for feature '['sex']':\n",
      "      \tCategories: [' Female' ' Male']\n",
      "    * Label Encoder for feature '['workclass']':\n",
      "      \tCategories: [' ?' ' Federal-gov' ' Local-gov' ' Never-worked' ' Private'\n",
      "       ' Self-emp-inc' ' Self-emp-not-inc' ' State-gov' ' Without-pay']\n",
      "    * RandomForestClassifier(max_depth=7, random_state=42)\n",
      "\n",
      "Test AUC 0.9101\n"
     ]
    }
   ],
   "source": [
    "rf_model.fit(train_X, train_y)\n",
    "test_preds = rf_model.predict_proba(test_X)\n",
    "rf_test_auc = roc_auc_score(test_y, test_preds[:,1])\n",
    "print(rf_model)\n",
    "print(\"Test AUC\", round(rf_test_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Show Pipeline Transformations\n",
    "After we fit our pipeline, we can easily show the original dataset along with the new transformed features, that get created by our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>marital-status_TE_</th>\n",
       "      <th>sex _OHE_ Female_</th>\n",
       "      <th>sex _OHE_ Male_</th>\n",
       "      <th>workclass _LE_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.045961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.446848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.446848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.446848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.045961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass  fnlwgt      education  education-num       marital-status  \\\n",
       "0   25     Private  226802           11th              7        Never-married   \n",
       "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
       "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
       "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
       "4   18           ?  103497   Some-college             10        Never-married   \n",
       "\n",
       "           occupation relationship    race      sex  capital-gain  \\\n",
       "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
       "1     Farming-fishing      Husband   White     Male             0   \n",
       "2     Protective-serv      Husband   White     Male             0   \n",
       "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
       "4                   ?    Own-child   White   Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country  marital-status_TE_  \\\n",
       "0             0              40   United-States            0.045961   \n",
       "1             0              50   United-States            0.446848   \n",
       "2             0              40   United-States            0.446848   \n",
       "3             0              40   United-States            0.446848   \n",
       "4             0              30   United-States            0.045961   \n",
       "\n",
       "   sex _OHE_ Female_  sex _OHE_ Male_  workclass _LE_  \n",
       "0                0.0              1.0               4  \n",
       "1                0.0              1.0               4  \n",
       "2                0.0              1.0               2  \n",
       "3                0.0              1.0               4  \n",
       "4                1.0              0.0               0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.transform(test_X).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Show only features used by the model\n",
    "\n",
    "We can also subset transformed dataset to only features used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>education-num</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>marital-status_TE_</th>\n",
       "      <th>sex _OHE_ Female_</th>\n",
       "      <th>sex _OHE_ Male_</th>\n",
       "      <th>workclass _LE_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>0.045961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>0.446848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>0.446848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0.446848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.045961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  capital-gain  capital-loss  education-num  hours-per-week  \\\n",
       "0   25             0             0              7              40   \n",
       "1   38             0             0              9              50   \n",
       "2   28             0             0             12              40   \n",
       "3   44          7688             0             10              40   \n",
       "4   18             0             0             10              30   \n",
       "\n",
       "   marital-status_TE_  sex _OHE_ Female_  sex _OHE_ Male_  workclass _LE_  \n",
       "0            0.045961                0.0              1.0               4  \n",
       "1            0.446848                0.0              1.0               4  \n",
       "2            0.446848                0.0              1.0               2  \n",
       "3            0.446848                0.0              1.0               4  \n",
       "4            0.045961                1.0              0.0               0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.transform(test_X)[rf_model.used_cols].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Show model feature importance\n",
    "\n",
    "There is also a possibility to show the feature importance of our model by calling it's `varimp` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marital-status_TE_</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-gain</th>\n",
       "      <td>0.828309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education-num</th>\n",
       "      <td>0.578073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.298391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-loss</th>\n",
       "      <td>0.172780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours-per-week</th>\n",
       "      <td>0.152922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex _OHE_ Male_</th>\n",
       "      <td>0.050922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex _OHE_ Female_</th>\n",
       "      <td>0.040938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workclass _LE_</th>\n",
       "      <td>0.015075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Importance\n",
       "Feature                       \n",
       "marital-status_TE_    1.000000\n",
       "capital-gain          0.828309\n",
       "education-num         0.578073\n",
       "age                   0.298391\n",
       "capital-loss          0.172780\n",
       "hours-per-week        0.152922\n",
       "sex _OHE_ Male_       0.050922\n",
       "sex _OHE_ Female_     0.040938\n",
       "workclass _LE_        0.015075"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.varimp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering Search\n",
    "\n",
    "In practice we might not know what the data processing pipeline should be, we want the one that optimizes our selected metric the most for a given model/algorithm. For that reason `barusini.tabular` implements function `feature_engineering` that aims to find such data pipeline.\n",
    "\n",
    "The parameters of this function are:\n",
    "\n",
    "- `X` - pd.DataFrame - input data\n",
    "- `y` - pd.Series - input target\n",
    "- `model_path` - optional string - if provided, the pipeline will be saved as a pickle to this path\n",
    "- `imputation_stage` - bool - whether we allow imputation of missing values\n",
    "- `subset_stage` - bool - whether we allow subsetting unimportant features\n",
    "- `encode_stage` - bool - whether we allow encoding string/text columns\n",
    "- `recode_stage` - bool - whether we allow encoding numeric columns and treating them as categorical\n",
    "- `allowed_transformers` - list of transformers - what are the transformers that can be used by encode/recode stages\n",
    "- `estimator` - scikit-learn API compatible model - model that will be used for predictions\n",
    "- `cv` - scikit-learn like validation object\n",
    "- `metric` -  scikit-learn like metric function\n",
    "- `maximize` - bool - whether the `metric` should be maximized (True) or minimized (False)\n",
    "- `proba` -  bool - whether we should use predict_proba or predict function of the model\n",
    " \n",
    "This function finds the best pipeline for a given model in a deterministic and reproducible way, as long as the model and the transformers allowed are all reproducible. In this example we are searching for the optimal data pipeline for xgboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of stage Basic Preprocessing: 0.28339695930480957 seconds\n",
      "Duration of stage Find Best Imputation: 0.0007910728454589844 seconds\n",
      "-------------------------------- Starting stage Finding best subset --------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: stdin isn't a terminal\n",
      "stty: stdin isn't a terminal\n",
      "stty: stdin isn't a terminal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE 0.8707828738592046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: stdin isn't a terminal         \n",
      "stty: stdin isn't a terminal\n",
      "stty: stdin isn't a terminal\n",
      "stty: stdin isn't a terminal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BEST 0.875091868037608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: stdin isn't a terminal         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL BEST 0.8707828738592046\n",
      "NEW BEST 0.875091868037608\n",
      "DIFF 0.004308994178403358\n",
      "Dropped ['fnlwgt']\n",
      "New []\n",
      "-------------------------------- Stage Finding best subset finished --------------------------------\n",
      "Duration of stage Find Best Subset: 3.968306064605713 seconds\n",
      "Encoding stage for  Index(['workclass', 'education', 'marital-status', 'occupation',\n",
      "       'relationship', 'race', 'sex', 'native-country'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders for workclass: ['MeanTargetEncoder', 'CustomLabelEncoder']\n",
      "-------------------------- Starting stage Encoding categoricals workclass --------------------------\n",
      "BASE 0.875091868037608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: stdin isn't a terminal\n",
      "stty: stdin isn't a terminal\n",
      " 12%|█▎        | 1/8 [00:02<00:15,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BEST 0.8768288094801094\n",
      "ORIGINAL BEST 0.875091868037608\n",
      "NEW BEST 0.8768288094801094\n",
      "DIFF 0.0017369414425014718\n",
      "Dropped []\n",
      "New ['workclass _LE_']\n",
      "-------------------------- Stage Encoding categoricals workclass finished --------------------------\n",
      "Encoders for education: ['MeanTargetEncoder']\n",
      "-------------------------- Starting stage Encoding categoricals education --------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: stdin isn't a terminal\n",
      "stty: stdin isn't a terminal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE 0.8768288094801094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:03<00:09,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL BEST 0.8768288094801094\n",
      "NEW BEST 0.8768288094801094\n",
      "DIFF 0.0\n",
      "Dropped []\n",
      "New []\n",
      "-------------------------- Stage Encoding categoricals education finished --------------------------\n",
      "Encoders for marital-status: ['MeanTargetEncoder', 'CustomLabelEncoder']\n",
      "------------------------ Starting stage Encoding categoricals marital-status ------------------------\n",
      "BASE 0.8768288094801094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:04<00:06,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BEST 0.9205910007774097\n",
      "ORIGINAL BEST 0.8768288094801094\n",
      "NEW BEST 0.9205910007774097\n",
      "DIFF 0.0437621912973003\n",
      "Dropped []\n",
      "New ['marital-status_TE_']\n",
      "------------------------ Stage Encoding categoricals marital-status finished ------------------------\n",
      "Encoders for occupation: ['MeanTargetEncoder']\n",
      "-------------------------- Starting stage Encoding categoricals occupation --------------------------\n",
      "BASE 0.9205910007774097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:05<00:04,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BEST 0.9248273363988192\n",
      "ORIGINAL BEST 0.9205910007774097\n",
      "NEW BEST 0.9248273363988192\n",
      "DIFF 0.004236335621409459\n",
      "Dropped []\n",
      "New ['occupation_TE_']\n",
      "-------------------------- Stage Encoding categoricals occupation finished --------------------------\n",
      "Encoders for relationship: ['MeanTargetEncoder', 'CustomLabelEncoder']\n",
      "------------------------- Starting stage Encoding categoricals relationship -------------------------\n",
      "BASE 0.9248273363988192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:06<00:03,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BEST 0.926222354862034\n",
      "ORIGINAL BEST 0.9248273363988192\n",
      "NEW BEST 0.926222354862034\n",
      "DIFF 0.00139501846321477\n",
      "Dropped []\n",
      "New ['relationship_TE_']\n",
      "------------------------- Stage Encoding categoricals relationship finished -------------------------\n",
      "Encoders for race: ['MeanTargetEncoder', 'CustomLabelEncoder']\n",
      "----------------------------- Starting stage Encoding categoricals race -----------------------------\n",
      "BASE 0.926222354862034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:07<00:02,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL BEST 0.926222354862034\n",
      "NEW BEST 0.926222354862034\n",
      "DIFF 0.0\n",
      "Dropped []\n",
      "New []\n",
      "----------------------------- Stage Encoding categoricals race finished -----------------------------\n",
      "Encoders for sex: ['CustomLabelEncoder']\n",
      "----------------------------- Starting stage Encoding categoricals sex -----------------------------\n",
      "BASE 0.926222354862034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BEST 0.9268737735900122\n",
      "ORIGINAL BEST 0.926222354862034\n",
      "NEW BEST 0.9268737735900122\n",
      "DIFF 0.000651418727978248\n",
      "Dropped []\n",
      "New ['sex _LE_']\n",
      "----------------------------- Stage Encoding categoricals sex finished -----------------------------\n",
      "Encoders for native-country: ['MeanTargetEncoder']\n",
      "------------------------ Starting stage Encoding categoricals native-country ------------------------\n",
      "BASE 0.9268737735900122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL BEST 0.9268737735900122\n",
      "NEW BEST 0.9268737735900122\n",
      "DIFF 0.0\n",
      "Dropped []\n",
      "New []\n",
      "------------------------ Stage Encoding categoricals native-country finished ------------------------\n",
      "Duration of stage Encode categoricals: 10.05614709854126 seconds\n",
      "Trying to recode following categorical values: ['education-num']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders for education-num: ['MeanTargetEncoder']\n",
      "------------------------------- Starting stage Recoding education-num -------------------------------\n",
      "BASE 0.9268737735900122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL BEST 0.9268737735900122\n",
      "NEW BEST 0.9268737735900122\n",
      "DIFF 0.0\n",
      "Dropped []\n",
      "New []\n",
      "------------------------------- Stage Recoding education-num finished -------------------------------\n",
      "Duration of stage Recode categoricals: 0.8671061992645264 seconds\n",
      "Final features: ['age', 'capital-gain', 'capital-loss', 'education', 'education-num', 'fnlwgt', 'hours-per-week', 'marital-status', 'marital-status_TE_', 'native-country', 'occupation', 'occupation_TE_', 'race', 'relationship', 'relationship_TE_', 'sex', 'sex _LE_', 'workclass', 'workclass _LE_']\n",
      "Duration of stage Feature engineering: 15.224539279937744 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "xgb_model = feature_engineering(\n",
    "    X=train_X,\n",
    "    y=train_y,\n",
    "    model_path=None,  # do not save model to pickle file\n",
    "    imputation_stage=True,  # try to impute missing values\n",
    "    subset_stage=True,  # try subseting input features\n",
    "    encode_stage=True,  # try encoding categorical features\n",
    "    recode_stage=True,  # try recoding numeric features\n",
    "    allowed_transformers=(  # allow only basic categorical encoders for encode/recode stages\n",
    "        CustomLabelEncoder,\n",
    "        CustomLabelEncoder,\n",
    "        MeanTargetEncoder,\n",
    "    ),\n",
    "    estimator=XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),  # use xgboost model\n",
    "    cv=StratifiedKFold(n_splits=3, random_state=42, shuffle=True),  # make stratified CV\n",
    "    metric=roc_auc_score,  # optimize AUC\n",
    "    classification=True,  # problem is classification\n",
    "    maximize=True,  # maximize metric (AUC)\n",
    "    proba=True,  # use soft predictions for metric (AUC)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC: 0.9267\n"
     ]
    }
   ],
   "source": [
    "xgb_cv_auc = cross_val_score(\n",
    "    model=xgb_model,\n",
    "    X=train_X,\n",
    "    y=train_y,\n",
    "    cv=StratifiedKFold(),\n",
    "    scoring=roc_auc_score,\n",
    "    n_jobs=1,\n",
    "    proba=True,\n",
    ")\n",
    "print(\"CV AUC:\", round(xgb_cv_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate The Pipeline\n",
    "\n",
    "Here we define the `pipeline_summary` function printing out the Pipeline along with the test ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline (10 Transformers):\n",
      "    * Identity: ['age']\n",
      "    * Identity: ['education-num']\n",
      "    * Identity: ['capital-gain']\n",
      "    * Identity: ['capital-loss']\n",
      "    * Identity: ['hours-per-week']\n",
      "    * Label Encoder for feature '['workclass']':\n",
      "      \tCategories: [' ?' ' Federal-gov' ' Local-gov' ' Never-worked' ' Private'\n",
      "       ' Self-emp-inc' ' Self-emp-not-inc' ' State-gov' ' Without-pay']\n",
      "    * Target encoder for feature '['marital-status']'\n",
      "          * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                            ('mean', LinearRegression())])\n",
      "    * Target encoder for feature '['occupation']'\n",
      "          * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                            ('mean', LinearRegression())])\n",
      "    * Target encoder for feature '['relationship']'\n",
      "          * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                            ('mean', LinearRegression())])\n",
      "    * Label Encoder for feature '['sex']':\n",
      "      \tCategories: [' Female' ' Male']\n",
      "    * XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='logloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)\n",
      "\n",
      "Test AUC: 0.9254\n"
     ]
    }
   ],
   "source": [
    "def pipeline_summary(pipeline, test_X, test_y, verbose=True):\n",
    "    test_preds = pipeline.predict_proba(test_X)\n",
    "    test_auc = roc_auc_score(test_y, test_preds[:,1])\n",
    "    if verbose:\n",
    "        print(pipeline)\n",
    "        print(\"Test AUC:\", round(test_auc, 4))\n",
    "    return test_auc\n",
    "\n",
    "xgb_test_auc = pipeline_summary(xgb_model, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tune Hyper-Parameters of Pipeline's Model\n",
    "\n",
    "First, we will copy our xgboost pipeline, keep the data processing the same and replace the model with a lightgbm model. Then we will use Pipeline's `tune` method to tune the hyper-parameters of the Pipeline's model. Note that calling the `tune` function changes the model and also refit's the Pipeline.\n",
    "\n",
    "The `tune` function takes following arguments:\n",
    "\n",
    "- `X` - pd.DataFrame - input data\n",
    "- `y` - pd.Series - input target\n",
    "- `cv` - scikit-learn like validation object\n",
    "- `score` -  scikit-learn like metric function\n",
    "- `maximize` - bool - whether the `metric` should be maximized (True) or minimized (False)\n",
    "- `probability` -  bool - whether we should use predict_proba or predict function of the model\n",
    "- `static_params` - dict - static/fixed hyper-parameters\n",
    "- `params` - dict - hyper-parameters to tune, these are defined as following:\n",
    "    - *key* - name of the hyper-parameter\n",
    "    - *value* - tuple with 3 entries\n",
    "        - distribution type (LOG, LOGINT, UNIFORM)\n",
    "        - min possible value\n",
    "        - max possible value\n",
    "- `n_trials` - int - number of different combinations tried\n",
    "- `n_jobs` - int - number of processes used\n",
    "- `seed` - optional int - seed to make tuning reproducible\n",
    " \n",
    "After tuning the Pipeline, we will print it out together with the test ROC AUC score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-06 15:46:50,887] A new study created in memory with name: no-name-ad3d19ff-6644-4bfd-b82f-9d7bd27583fc\n",
      "[I 2023-11-06 15:46:56,455] Trial 0 finished with value: -0.904276103041477 and parameters: {'min_child_samples': 9, 'num_leaves': 3003, 'learning_rate': 0.4570563099801455, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746}. Best is trial 0 with value: -0.904276103041477.\n",
      "[I 2023-11-06 15:46:57,074] Trial 1 finished with value: -0.6274178669998255 and parameters: {'min_child_samples': 2, 'num_leaves': 11, 'learning_rate': 2.1423021757741068, 'subsample': 0.8404460046972835, 'colsample_bytree': 0.8832290311184181}. Best is trial 0 with value: -0.904276103041477.\n",
      "[I 2023-11-06 15:46:58,872] Trial 2 finished with value: -0.6506231790699374 and parameters: {'min_child_samples': 1, 'num_leaves': 3389, 'learning_rate': 1.452824663751602, 'subsample': 0.6849356442713105, 'colsample_bytree': 0.6727299868828402}. Best is trial 0 with value: -0.904276103041477.\n",
      "[I 2023-11-06 15:46:59,998] Trial 3 finished with value: -0.9271328201194534 and parameters: {'min_child_samples': 2, 'num_leaves': 51, 'learning_rate': 0.042051564509138675, 'subsample': 0.7727780074568463, 'colsample_bytree': 0.7164916560792167}. Best is trial 3 with value: -0.9271328201194534.\n",
      "[I 2023-11-06 15:47:00,872] Trial 4 finished with value: -0.90893030896429 and parameters: {'min_child_samples': 52, 'num_leaves': 18, 'learning_rate': 0.0028888383623653178, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144}. Best is trial 3 with value: -0.9271328201194534.\n",
      "[I 2023-11-06 15:47:01,873] Trial 5 finished with value: -0.919173967928105 and parameters: {'min_child_samples': 195, 'num_leaves': 26, 'learning_rate': 0.03725393839578886, 'subsample': 0.836965827544817, 'colsample_bytree': 0.6185801650879991}. Best is trial 3 with value: -0.9271328201194534.\n",
      "[I 2023-11-06 15:47:02,737] Trial 6 finished with value: -0.8937482768362631 and parameters: {'min_child_samples': 51, 'num_leaves': 22, 'learning_rate': 0.00021147447960615738, 'subsample': 0.9795542149013333, 'colsample_bytree': 0.9862528132298237}. Best is trial 3 with value: -0.9271328201194534.\n",
      "[I 2023-11-06 15:47:03,827] Trial 7 finished with value: -0.9081406150105515 and parameters: {'min_child_samples': 233, 'num_leaves': 51, 'learning_rate': 0.0003078651783619622, 'subsample': 0.8736932106048627, 'colsample_bytree': 0.7760609974958406}. Best is trial 3 with value: -0.9271328201194534.\n",
      "[I 2023-11-06 15:47:05,375] Trial 8 finished with value: -0.9215234939445757 and parameters: {'min_child_samples': 1, 'num_leaves': 170, 'learning_rate': 0.00014857392806279257, 'subsample': 0.9637281608315128, 'colsample_bytree': 0.7035119926400067}. Best is trial 3 with value: -0.9271328201194534.\n",
      "[I 2023-11-06 15:47:06,550] Trial 9 finished with value: -0.92459418462162 and parameters: {'min_child_samples': 77, 'num_leaves': 53, 'learning_rate': 0.039841905944346875, 'subsample': 0.8186841117373118, 'colsample_bytree': 0.6739417822102108}. Best is trial 3 with value: -0.9271328201194534.\n",
      "[I 2023-11-06 15:47:07,209] Trial 10 finished with value: -0.7146625782285382 and parameters: {'min_child_samples': 6, 'num_leaves': 278, 'learning_rate': 7.041830285016017, 'subsample': 0.6071847502459278, 'colsample_bytree': 0.7438461564020347}. Best is trial 3 with value: -0.9271328201194534.\n",
      "[I 2023-11-06 15:47:08,451] Trial 11 finished with value: -0.9278599565226183 and parameters: {'min_child_samples': 13, 'num_leaves': 79, 'learning_rate': 0.0553867538910981, 'subsample': 0.764260120678709, 'colsample_bytree': 0.6028490459729081}. Best is trial 11 with value: -0.9278599565226183.\n",
      "[I 2023-11-06 15:47:09,836] Trial 12 finished with value: -0.9234579283898551 and parameters: {'min_child_samples': 12, 'num_leaves': 125, 'learning_rate': 0.15667777352367712, 'subsample': 0.7367802452605878, 'colsample_bytree': 0.6075269693526745}. Best is trial 11 with value: -0.9278599565226183.\n",
      "[I 2023-11-06 15:47:10,624] Trial 13 finished with value: -0.9045905056154501 and parameters: {'min_child_samples': 4, 'num_leaves': 9, 'learning_rate': 0.004680946144628577, 'subsample': 0.7604603292118235, 'colsample_bytree': 0.6015969889135928}. Best is trial 11 with value: -0.9278599565226183.\n",
      "[I 2023-11-06 15:47:11,792] Trial 14 finished with value: -0.9268453897012179 and parameters: {'min_child_samples': 19, 'num_leaves': 73, 'learning_rate': 0.11557592172110982, 'subsample': 0.705456779118783, 'colsample_bytree': 0.7320137184311114}. Best is trial 11 with value: -0.9278599565226183.\n",
      "[I 2023-11-06 15:47:12,770] Trial 15 finished with value: -0.8981412351431299 and parameters: {'min_child_samples': 771, 'num_leaves': 196, 'learning_rate': 0.008616562973972916, 'subsample': 0.7747658916716722, 'colsample_bytree': 0.6469952889624938}. Best is trial 11 with value: -0.9278599565226183.\n",
      "[I 2023-11-06 15:47:14,993] Trial 16 finished with value: -0.9231157643066958 and parameters: {'min_child_samples': 4, 'num_leaves': 410, 'learning_rate': 0.014405784989686973, 'subsample': 0.6617843657491931, 'colsample_bytree': 0.7087321927571174}. Best is trial 11 with value: -0.9278599565226183.\n",
      "[I 2023-11-06 15:47:16,108] Trial 17 finished with value: -0.924361671018945 and parameters: {'min_child_samples': 2, 'num_leaves': 85, 'learning_rate': 0.15738308874337398, 'subsample': 0.7873311418819124, 'colsample_bytree': 0.8325867073945821}. Best is trial 11 with value: -0.9278599565226183.\n",
      "[I 2023-11-06 15:47:17,155] Trial 18 finished with value: -0.9178778547251213 and parameters: {'min_child_samples': 21, 'num_leaves': 38, 'learning_rate': 0.0019691648906460454, 'subsample': 0.7176966401771596, 'colsample_bytree': 0.635237148802722}. Best is trial 11 with value: -0.9278599565226183.\n",
      "[I 2023-11-06 15:47:19,672] Trial 19 finished with value: -0.9219195043149259 and parameters: {'min_child_samples': 9, 'num_leaves': 486, 'learning_rate': 0.01943676544859357, 'subsample': 0.9046650933240872, 'colsample_bytree': 0.693229995692692}. Best is trial 11 with value: -0.9278599565226183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 20 trials the best score is -0.9278599565226183 with params {'min_child_samples': 13, 'num_leaves': 79, 'learning_rate': 0.0553867538910981, 'subsample': 0.764260120678709, 'colsample_bytree': 0.6028490459729081}\n",
      "Pipeline (10 Transformers):\n",
      "    * Identity: ['age']\n",
      "    * Identity: ['education-num']\n",
      "    * Identity: ['capital-gain']\n",
      "    * Identity: ['capital-loss']\n",
      "    * Identity: ['hours-per-week']\n",
      "    * Label Encoder for feature '['workclass']':\n",
      "      \tCategories: [' ?' ' Federal-gov' ' Local-gov' ' Never-worked' ' Private'\n",
      "       ' Self-emp-inc' ' Self-emp-not-inc' ' State-gov' ' Without-pay']\n",
      "    * Target encoder for feature '['marital-status']'\n",
      "          * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                            ('mean', LinearRegression())])\n",
      "    * Target encoder for feature '['occupation']'\n",
      "          * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                            ('mean', LinearRegression())])\n",
      "    * Target encoder for feature '['relationship']'\n",
      "          * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                            ('mean', LinearRegression())])\n",
      "    * Label Encoder for feature '['sex']':\n",
      "      \tCategories: [' Female' ' Male']\n",
      "    * LGBMClassifier(colsample_bytree=0.6028490459729081,\n",
      "               learning_rate=0.0553867538910981, min_child_samples=13, n_jobs=1,\n",
      "               num_leaves=79, seed=42, subsample=0.764260120678709, verbose=-1)\n",
      "\n",
      "Test AUC: 0.9276\n",
      "CV AUC: 0.928\n"
     ]
    }
   ],
   "source": [
    "lgb_model = copy.deepcopy(xgb_model)\n",
    "lgb_model.model = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "lgb_model.tune(\n",
    "    X=train_X,\n",
    "    y=train_y,\n",
    "    cv=StratifiedKFold(n_splits=3, random_state=42, shuffle=True),\n",
    "    score=roc_auc_score,\n",
    "    probability=True,\n",
    "    maximize=True,\n",
    "    static_params={\"n_estimators\": 100, \"seed\": 42, \"n_jobs\": 1, \"verbose\":-1},\n",
    "    # hyper parameters and tuning space\n",
    "    params={\n",
    "        \"min_child_samples\": (LOGINT, (1, 1000)),\n",
    "        \"num_leaves\": (LOGINT, (2 ** 3, 2 ** 12)),\n",
    "        \"learning_rate\": (LOG, (1e-4, 1e1)),\n",
    "        \"subsample\": (UNIFORM, (0.6, 1)),\n",
    "        \"colsample_bytree\": (UNIFORM, (0.6, 1)),\n",
    "    },\n",
    "    attributes_to_monitor={},\n",
    "    n_trials=20,\n",
    "    n_jobs=1,  # make tuning deterministic\n",
    "    seed=42,  # make tuning deterministic\n",
    ")\n",
    "\n",
    "lgb_cv_auc = cross_val_score(\n",
    "    model=lgb_model,\n",
    "    X=train_X,\n",
    "    y=train_y,\n",
    "    cv=StratifiedKFold(),\n",
    "    scoring=roc_auc_score,\n",
    "    n_jobs=1,\n",
    "    proba=True,\n",
    ")\n",
    "\n",
    "lgb_test_auc = pipeline_summary(lgb_model, test_X, test_y)\n",
    "print(\"CV AUC:\", round(lgb_cv_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ensembling Multiple Pipelines\n",
    "\n",
    "We can define multiple Pipelines, fit each of them individually and then create additional 2-nd level model (meta model) taking all of their predictions as input generating new predictions to achieve better performance. This process is called ensembling and can be done by the `Ensemble` object.\n",
    "\n",
    "`Ensemble` object is defined by providing:\n",
    "- `pipelines` - list of Pipelines to use as 1-st level models\n",
    "- `meta` - optional model to use as 2-nd level model - if it is not provided it will be a simple weighted average where all weights are positive and sum to 1\n",
    "- `cv` - scikit-learn like validation object that defines how the Out-Of-Fold (OOF) predictions of 1-st level models will be generated to create features for 2-nd level model\n",
    "\n",
    "The ensemble object can be fitted or (cross) validated just like a regular pipeline. Since fitting an ensemble object requires generating OOF predictions, some form of validation is done during fitting, so computing cross-validation score of an ensemble object results in nested (cross) validation, therefor it can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.72s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1611.95it/s]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 39444.87it/s]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.76s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 42083.32it/s]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.76s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 42799.02it/s]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.76s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 35345.26it/s]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.98s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 33376.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble,  (WeightedAverage, 3 Pipelines):\n",
      "    * Pipeline (8 Transformers):\n",
      "          * Identity: ['age']\n",
      "          * Identity: ['education-num']\n",
      "          * Identity: ['capital-gain']\n",
      "          * Identity: ['capital-loss']\n",
      "          * Identity: ['hours-per-week']\n",
      "          * Target encoder for feature '['marital-status']'\n",
      "                * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                                  ('mean', LinearRegression())])\n",
      "          * One Hot Encoder for feature '['sex']':\n",
      "            \tCategories: [' Female' ' Male']\n",
      "          * Label Encoder for feature '['workclass']':\n",
      "            \tCategories: [' ?' ' Federal-gov' ' Local-gov' ' Never-worked' ' Private'\n",
      "             ' Self-emp-inc' ' Self-emp-not-inc' ' State-gov' ' Without-pay']\n",
      "          * RandomForestClassifier(max_depth=7, random_state=42)\n",
      "      \n",
      "    * Pipeline (10 Transformers):\n",
      "          * Identity: ['age']\n",
      "          * Identity: ['education-num']\n",
      "          * Identity: ['capital-gain']\n",
      "          * Identity: ['capital-loss']\n",
      "          * Identity: ['hours-per-week']\n",
      "          * Label Encoder for feature '['workclass']':\n",
      "            \tCategories: [' ?' ' Federal-gov' ' Local-gov' ' Never-worked' ' Private'\n",
      "             ' Self-emp-inc' ' Self-emp-not-inc' ' State-gov' ' Without-pay']\n",
      "          * Target encoder for feature '['marital-status']'\n",
      "                * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                                  ('mean', LinearRegression())])\n",
      "          * Target encoder for feature '['occupation']'\n",
      "                * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                                  ('mean', LinearRegression())])\n",
      "          * Target encoder for feature '['relationship']'\n",
      "                * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                                  ('mean', LinearRegression())])\n",
      "          * Label Encoder for feature '['sex']':\n",
      "            \tCategories: [' Female' ' Male']\n",
      "          * XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                    colsample_bylevel=None, colsample_bynode=None,\n",
      "                    colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "                    enable_categorical=False, eval_metric='logloss',\n",
      "                    feature_types=None, gamma=None, grow_policy=None,\n",
      "                    importance_type=None, interaction_constraints=None,\n",
      "                    learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "                    max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "                    max_leaves=None, min_child_weight=None, missing=nan,\n",
      "                    monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "                    n_jobs=None, num_parallel_tree=None, random_state=None, ...)\n",
      "      \n",
      "    * Pipeline (10 Transformers):\n",
      "          * Identity: ['age']\n",
      "          * Identity: ['education-num']\n",
      "          * Identity: ['capital-gain']\n",
      "          * Identity: ['capital-loss']\n",
      "          * Identity: ['hours-per-week']\n",
      "          * Label Encoder for feature '['workclass']':\n",
      "            \tCategories: [' ?' ' Federal-gov' ' Local-gov' ' Never-worked' ' Private'\n",
      "             ' Self-emp-inc' ' Self-emp-not-inc' ' State-gov' ' Without-pay']\n",
      "          * Target encoder for feature '['marital-status']'\n",
      "                * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                                  ('mean', LinearRegression())])\n",
      "          * Target encoder for feature '['occupation']'\n",
      "                * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                                  ('mean', LinearRegression())])\n",
      "          * Target encoder for feature '['relationship']'\n",
      "                * Pipeline(steps=[('enc', OneHotEncoder(handle_unknown='ignore')),\n",
      "                                  ('mean', LinearRegression())])\n",
      "          * Label Encoder for feature '['sex']':\n",
      "            \tCategories: [' Female' ' Male']\n",
      "          * LGBMClassifier(colsample_bytree=0.6028490459729081,\n",
      "                     learning_rate=0.0553867538910981, min_child_samples=13, n_jobs=1,\n",
      "                     num_leaves=79, seed=42, subsample=0.764260120678709, verbose=-1)\n",
      "      \n",
      "    * Weighted average: [0.0472943  0.24308002 0.70962567]\n",
      "\n",
      "Test AUC: 0.9278\n",
      "Ensemble CV AUC: 0.9284\n"
     ]
    }
   ],
   "source": [
    "meta = WeightedAverage(num_classes=train_y.nunique(), tol=1e-14)\n",
    "ensemble = Ensemble(pipelines=[rf_model, xgb_model, lgb_model], meta=meta)\n",
    "ensemble_cv_auc = cross_val_score(\n",
    "    model=ensemble,\n",
    "    X=train_X,\n",
    "    y=train_y,\n",
    "    cv=StratifiedKFold(),\n",
    "    scoring=roc_auc_score,\n",
    "    n_jobs=1,\n",
    "    proba=True,\n",
    ")\n",
    "\n",
    "ensemble.fit(train_X, train_y)\n",
    "ensemble_test_auc = pipeline_summary(ensemble, test_X, test_y)\n",
    "print(\"Ensemble CV AUC:\", round(ensemble_cv_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Show Ensemble Weights\n",
    "\n",
    "If we use default enseble meta algorithm (simple weighted average) we can see the weights of the individual Pipelines. We can also verify that the weights sum up to 1 and are non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0472943 , 0.24308002, 0.70962567])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert np.isclose(ensemble.meta.weights.sum(), 1)\n",
    "assert all(ensemble.meta.weights >= 0)\n",
    "ensemble.meta.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Comparison\n",
    "\n",
    "Finally we can compare the cross-validation and test performance of the pipelines we created and their ensemble. We can see that the ensemble achieves better performance, it does however consist of three individual pipelines so it is more complex and slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>CV AUC</th>\n",
       "      <th>Test AUC</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0.928355</td>\n",
       "      <td>0.927848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.928018</td>\n",
       "      <td>0.927578</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.926663</td>\n",
       "      <td>0.925415</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.911471</td>\n",
       "      <td>0.910122</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model    CV AUC  Test AUC  rank\n",
       "3       Ensemble  0.928355  0.927848     1\n",
       "2       LightGBM  0.928018  0.927578     2\n",
       "1        XGBoost  0.926663  0.925415     3\n",
       "0  Random Forest  0.911471  0.910122     4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_data = {\n",
    "    \"model\": [\"Random Forest\", \"XGBoost\", \"LightGBM\", \"Ensemble\"],\n",
    "    \"CV AUC\": [rf_cv_auc, xgb_cv_auc, lgb_cv_auc, ensemble_cv_auc],\n",
    "    \"Test AUC\": [rf_test_auc, xgb_test_auc, lgb_test_auc, ensemble_test_auc],\n",
    "}\n",
    "\n",
    "scores = pd.DataFrame(scores_data).sort_values(\"Test AUC\", ascending=False)\n",
    "scores[\"rank\"] = range(1, scores.shape[0]+1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "barusini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
